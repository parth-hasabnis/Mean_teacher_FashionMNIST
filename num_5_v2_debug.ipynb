{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Dataset, Sampler, DataLoader, Subset, SubsetRandomSampler, BatchSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from helper_functions_2 import softmax_kl_loss, sigmoid_rampup, get_current_consistency_weight, linear_rampup, grouper, relabel_dataset\n",
    "import time\n",
    "\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=None\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=None\n",
    ")\n",
    "\n",
    "class_names = train_data.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_LABEL = -1\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "class Arguments():\n",
    "    def __init__(self, momentum, weight_decay, nesterov, epochs:int, consistency, exclude_unlabeled:bool, batch_size=64, labeled_batch_size=32, consistency_type='kl', lr=0.01, initial_lr=0.001, lr_rampup = 10, ema_decay=0.999, consistency_rampup=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "        self.nesterov = nesterov\n",
    "        self.epochs = epochs\n",
    "        self.consistency_type = consistency_type\n",
    "        self.initial_lr = initial_lr\n",
    "        self.lr_rampup = lr_rampup\n",
    "        self.consistency = consistency\n",
    "        self.ema_decay = ema_decay\n",
    "        self.labeled_batch_size = labeled_batch_size\n",
    "        self.exclude_unlabeled = exclude_unlabeled\n",
    "        self.batch_size = batch_size\n",
    "        self.consistency_rampup = consistency_rampup\n",
    "\n",
    "args = Arguments(lr=0.01, momentum=0, weight_decay=0, nesterov=False, epochs=4, exclude_unlabeled=True, consistency=0, batch_size=64, labeled_batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' train_loader = torch.utils.data.DataLoader(\\n        dataset= train_data,\\n        batch_size=args.batch_size,\\n        shuffle=False,\\n        pin_memory=True,\\n        drop_last=False)\\n\\neval_loader = torch.utils.data.DataLoader(\\n    dataset=test_data,\\n    batch_size=args.batch_size,\\n    shuffle=False,\\n    pin_memory=True,\\n    drop_last=False) '"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" train_loader = torch.utils.data.DataLoader(\n",
    "        dataset= train_data,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        drop_last=False)\n",
    "\n",
    "eval_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_data,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    drop_last=False) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' class CustomTrainDataset(Dataset):\\n\\n    def  __init__(self, train_data, labels=None):\\n        self.data = train_data\\n        self.labels = labels\\n\\n    def __len__(self):\\n        return len(self.data)\\n    \\n    def __getitem__(self, idx):\\n        img = self.data[idx]\\n        img = img.numpy().astype(np.uint8).astype(np.uint8)\\n        label = self.labels[idx]\\n\\n        return img, label '"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" class CustomTrainDataset(Dataset):\n",
    "\n",
    "    def  __init__(self, train_data, labels=None):\n",
    "        self.data = train_data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.data[idx]\n",
    "        img = img.numpy().astype(np.uint8).astype(np.uint8)\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return img, label \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' train_dataset = CustomTrainDataset(train_data.data, train_data.targets)\\ntrain_loader = DataLoader(train_dataset, BATCH_SIZE, False)\\nX, y = next(iter(train_loader))\\nX, y = next(iter(train_loader))\\nN = int(sqrt(BATCH_SIZE))\\n\\nfig, axs = plt.subplots(N, N, figsize=(15,15))\\nfor i in range(len(y)):\\n    img = X[i]\\n    label = y[i]\\n    ax = fig.add_subplot(N, N, i+1)\\n    ax.imshow(img.reshape(28, 28),cmap=\"gray\")\\n    ax.set_xticks([]) #set empty label for x axis\\n    ax.set_yticks([]) #set empty label for y axis\\n    ax.set_title(f\"{class_names[label]}\")\\n\\nplt.show() '"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" train_dataset = CustomTrainDataset(train_data.data, train_data.targets)\n",
    "train_loader = DataLoader(train_dataset, BATCH_SIZE, False)\n",
    "X, y = next(iter(train_loader))\n",
    "X, y = next(iter(train_loader))\n",
    "N = int(sqrt(BATCH_SIZE))\n",
    "\n",
    "fig, axs = plt.subplots(N, N, figsize=(15,15))\n",
    "for i in range(len(y)):\n",
    "    img = X[i]\n",
    "    label = y[i]\n",
    "    ax = fig.add_subplot(N, N, i+1)\n",
    "    ax.imshow(img.reshape(28, 28),cmap=\"gray\")\n",
    "    ax.set_xticks([]) #set empty label for x axis\n",
    "    ax.set_yticks([]) #set empty label for y axis\n",
    "    ax.set_title(f\"{class_names[label]}\")\n",
    "\n",
    "plt.show() \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # IS this the problem \\nif(args.exclude_unlabeled == False):\\n    labelled_train_data, unlabelled_train_data, labels_train, unlabels_train = train_test_split(train_data.data, train_data.targets, stratify=train_data.targets, test_size=split)\\nelse: # This is not the problem\\n    labelled_train_data = train_data.data\\n    labels_train = train_data.targets '"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = 1 - args.labeled_batch_size/args.batch_size\n",
    "NO_LABEL = -1\n",
    "BATCH_SIZE = args.batch_size\n",
    "\n",
    "\"\"\" # IS this the problem \n",
    "if(args.exclude_unlabeled == False):\n",
    "    labelled_train_data, unlabelled_train_data, labels_train, unlabels_train = train_test_split(train_data.data, train_data.targets, stratify=train_data.targets, test_size=split)\n",
    "else: # This is not the problem\n",
    "    labelled_train_data = train_data.data\n",
    "    labels_train = train_data.targets \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST CELL\n",
    "\n",
    "train_set_size = int(len(train_data) * 0.8)\n",
    "valid_set_size = len(train_data) - train_set_size\n",
    "labelled_data, unlabelled_data = torch.utils.data.random_split(train_data, [train_set_size, valid_set_size])\n",
    "\n",
    "data_0 = []\n",
    "labels_0 = []\n",
    "data_1 = []\n",
    "\n",
    "data_test = []\n",
    "labels_test = []\n",
    "\n",
    "for (X, y) in labelled_data:\n",
    "    data_0.append(X)\n",
    "    labels_0.append(y)\n",
    "\n",
    "for (X, y) in test_data:\n",
    "    data_test.append(X)\n",
    "    labels_test.append(y)\n",
    "\n",
    "for (X, y) in unlabelled_data:\n",
    "    data_1.append(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainDataset(Dataset):\n",
    "    def __init__(self, train_data, labels=None):\n",
    "        self.base_data = train_data\n",
    "        if(labels == None):\n",
    "            self.labels = torch.tensor([-1 for i in range(len(self.base_data))],dtype=torch.int64)\n",
    "        else:\n",
    "            self.labels = labels\n",
    "        super().__init__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.base_data[idx]\n",
    "        # img = img.unsqueeze(dim=0)\n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "\n",
    "labelled_dataset = CustomTrainDataset(train_data=data_0, labels=labels_0)\n",
    "if(args.exclude_unlabeled == False):\n",
    "    unlabelled_dataset = CustomTrainDataset(train_data=data_1)\n",
    "    train_dataset = torch.utils.data.ConcatDataset([labelled_dataset, unlabelled_dataset])\n",
    "else:\n",
    "    train_dataset = labelled_dataset\n",
    "\n",
    "test_dataset = CustomTrainDataset(train_data=data_test, labels=labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # TEST CELL\\n\\nlabelled_train_data, unlabelled_train_data, labels_train, unlabels_train = train_test_split(train_data.data, train_data.targets, stratify=train_data.targets, test_size=0.2)\\nlabelled_dataset = CustomTrainDataset(train_data=labelled_train_data, labels=labels_train)\\nunlabelled_dataset = CustomTrainDataset(train_data=unlabelled_train_data)\\ntrain_dataset = torch.utils.data.ConcatDataset([labelled_dataset, unlabelled_dataset]) '"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # TEST CELL\n",
    "\n",
    "labelled_train_data, unlabelled_train_data, labels_train, unlabels_train = train_test_split(train_data.data, train_data.targets, stratify=train_data.targets, test_size=0.2)\n",
    "labelled_dataset = CustomTrainDataset(train_data=labelled_train_data, labels=labels_train)\n",
    "unlabelled_dataset = CustomTrainDataset(train_data=unlabelled_train_data)\n",
    "train_dataset = torch.utils.data.ConcatDataset([labelled_dataset, unlabelled_dataset]) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # TEST CELL\\n\\n# train_loader = DataLoader(unlabelled_dataset, BATCH_SIZE, False)\\nX, y = next(iter(train_loader))\\n# X, y = next(iter(eval_loader))\\nN = int(sqrt(BATCH_SIZE))\\n\\nfig, axs = plt.subplots(N, N, figsize=(17,17))\\nfor i in range(len(y)):\\n    img = X[i]\\n    label = y[i]\\n    ax = fig.add_subplot(N, N, i+1)\\n    ax.imshow(img.reshape(28, 28),cmap=\"gray\")\\n    ax.set_xticks([]) #set empty label for x axis\\n    ax.set_yticks([]) #set empty label for y axis\\n    if label != -1:\\n        title = f\"{class_names[label]}\"\\n    else:\\n        title = \"NO_LABEL\"\\n    ax.set_title(title)\\n\\nplt.show() '"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # TEST CELL\n",
    "\n",
    "# train_loader = DataLoader(unlabelled_dataset, BATCH_SIZE, False)\n",
    "X, y = next(iter(train_loader))\n",
    "# X, y = next(iter(eval_loader))\n",
    "N = int(sqrt(BATCH_SIZE))\n",
    "\n",
    "fig, axs = plt.subplots(N, N, figsize=(17,17))\n",
    "for i in range(len(y)):\n",
    "    img = X[i]\n",
    "    label = y[i]\n",
    "    ax = fig.add_subplot(N, N, i+1)\n",
    "    ax.imshow(img.reshape(28, 28),cmap=\"gray\")\n",
    "    ax.set_xticks([]) #set empty label for x axis\n",
    "    ax.set_yticks([]) #set empty label for y axis\n",
    "    if label != -1:\n",
    "        title = f\"{class_names[label]}\"\n",
    "    else:\n",
    "        title = \"NO_LABEL\"\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.show() \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoStreamBatchSampler(Sampler):\n",
    "     \n",
    "    def __init__(self, primary_indices, secondary_indices, batch_size, secondary_batch_size):\n",
    "        self.primary_indices = primary_indices\n",
    "        self.secondary_indices = secondary_indices\n",
    "        self.secondary_batch_size = secondary_batch_size\n",
    "        self.primary_batch_size = batch_size - secondary_batch_size\n",
    "\n",
    "        assert len(self.primary_indices) >= self.primary_batch_size > 0\n",
    "        assert len(self.secondary_indices) >= self.secondary_batch_size > 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "            primary_iter = np.random.permutation(self.primary_indices)\n",
    "            secondary_iter = np.random.permutation(self.secondary_indices)\n",
    "            return (\n",
    "                primary_batch + secondary_batch\n",
    "                for (primary_batch, secondary_batch)\n",
    "                in  zip(grouper(primary_iter, self.primary_batch_size),\n",
    "                        grouper(secondary_iter, self.secondary_batch_size)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.primary_indices) // self.primary_batch_size\n",
    "\n",
    "\n",
    "def create_data_loaders(train_dataset, test_dataset, args):\n",
    "    \n",
    "    labeled_idxs, unlabeled_idxs = relabel_dataset(dataset=train_dataset)\n",
    "\n",
    "    if args.exclude_unlabeled:\n",
    "        sampler = SubsetRandomSampler(labeled_idxs)\n",
    "        batch_sampler = BatchSampler(sampler, args.batch_size, drop_last=True)\n",
    "        \n",
    "    else: \n",
    "        batch_sampler = TwoStreamBatchSampler(unlabeled_idxs, labeled_idxs, args.batch_size, args.labeled_batch_size)\n",
    "\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                batch_sampler=batch_sampler,\n",
    "                                                pin_memory=True)\n",
    "\n",
    "    eval_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        drop_last=False)\n",
    "    \n",
    "    return train_loader, eval_loader\n",
    "\n",
    "train_loader, eval_loader = create_data_loaders(labelled_dataset, test_dataset, args)\n",
    "    \n",
    "# labelled_dataset = CustomTrainDataset(train_data=labelled_train_data, labels=labels_train)\n",
    "# train_loader_custom = torch.utils.data.DataLoader(\n",
    "#         dataset= labelled_dataset,\n",
    "#         batch_size=args.batch_size,\n",
    "#         shuffle=False,\n",
    "#         pin_memory=True,\n",
    "#         drop_last=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # TEST CELL\\n\\ntrain_loader = torch.utils.data.DataLoader(\\n        dataset= train_data,\\n        batch_size=args.batch_size,\\n        shuffle=False,\\n        pin_memory=True,\\n        drop_last=False)\\n\\neval_loader = torch.utils.data.DataLoader(\\n    dataset=test_data,\\n    batch_size=args.batch_size,\\n    shuffle=False,\\n    pin_memory=True,\\n    drop_last=False)\\n\\n '"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # TEST CELL\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        dataset= train_data,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        drop_last=False)\n",
    "\n",
    "eval_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_data,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    drop_last=False)\n",
    "\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNSITModel_V2(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=(3,3),\n",
    "                      padding=1,\n",
    "                      stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=(3,3),\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,2))\n",
    "        )\n",
    "\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=(3,3),\n",
    "                      padding=1,\n",
    "                      stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=(3,3),\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,2))\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=hidden_units*7*7,\n",
    "                      out_features=output_shape)\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_block_2(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "torch.manual_seed(42)\n",
    "\n",
    "model_2 = FashionMNSITModel_V2(\n",
    "    input_shape=1,\n",
    "    hidden_units=10,\n",
    "    output_shape=len(class_names)\n",
    ")\n",
    "\n",
    "def create_models(input_shape: int, hidden_units: int, output_shape:int, ema=False):\n",
    "\n",
    "    model = FashionMNSITModel_V2(\n",
    "    input_shape=input_shape,\n",
    "    hidden_units=hidden_units,\n",
    "    output_shape=output_shape)\n",
    "\n",
    "    if ema:\n",
    "        for param in model.parameters():\n",
    "            param.detach_()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_student = create_models(input_shape=1, hidden_units=10, output_shape=len(class_names))\n",
    "model_teacher = create_models(input_shape=1, hidden_units=10, output_shape=len(class_names), ema=True)\n",
    "\n",
    "optimizer = torch.optim.SGD(model_student.parameters(), args.lr)\n",
    "optimizer_2 = torch.optim.SGD(params=model_2.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(output, target, args, train=False):\n",
    "\n",
    "    y_preds = torch.argmax(output, dim=1)\n",
    "\n",
    "    if(train):\n",
    "        y_preds = y_preds[:args.labeled_batch_size]\n",
    "        target = target[:args.labeled_batch_size]\n",
    "\n",
    "    res = sum(torch.eq(y_preds,target)).item() / len(output)\n",
    "    \n",
    "    return res\n",
    "\n",
    "def update_ema_variables(model, ema_model, alpha, global_step):\n",
    "    # Use the true average until the exponential average is more correct\n",
    "    alpha = min(1 - 1 / (global_step + 1), alpha)\n",
    "    for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n",
    "        ema_param.data.mul_(alpha).add_(1 - alpha, param.data)\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, batch_num, batches_in_epoch, args):\n",
    "    lr = args.lr\n",
    "    epoch = epoch + batch_num / batches_in_epoch\n",
    "\n",
    "    lr = linear_rampup(epoch, args.lr_rampup) * (args.lr - args.initial_lr) + args.initial_lr\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model_student, model_teacher, optimizer, epoch, args):\n",
    "    global global_step\n",
    "\n",
    "    # Is this the problem? maybe. consistency = 0. so we dont know\n",
    "    class_criterion = nn.CrossEntropyLoss(reduction=\"sum\",ignore_index=NO_LABEL)\n",
    "    if args.consistency_type == 'kl':\n",
    "         consistency_criterion = softmax_kl_loss\n",
    "\n",
    "    model_student.train()\n",
    "    model_teacher.train() # is this the problem - No\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for batch, (X, y) in enumerate(train_loader):\n",
    "\n",
    "        # Adjust learning rate for minibatach - read more on this and minibatch/batch training\n",
    "        # Is this the problem? maybe. consistency = 0. so we dont know\n",
    "        if(args.exclude_unlabeled == False):\n",
    "            adjust_learning_rate(optimizer, epoch, batch, len(train_loader), args)\n",
    "\n",
    "        # Add noise to student and teacher inputs: ideally data should be augmented in the data loader. look more into it\n",
    "        student_input_var = X + 0.01*torch.randn(size=X.shape) # Not the problem\n",
    "        #student_input_var = student_input_var.unsqueeze(dim=1)\n",
    "        teacher_input_var = X + 0.01*torch.randn(size=X.shape) # Not the problem\n",
    "        #teacher_input_var = teacher_input_var.unsqueeze(dim=1)\n",
    "\n",
    "\n",
    "        minibatch_size = len(y)\n",
    "\n",
    "        # Forward Pass\n",
    "        student_out = model_student(student_input_var)\n",
    "        teacher_out = model_teacher(teacher_input_var) # is this the problem - No\n",
    "        \n",
    "        teacher_logit = teacher_out.detach().data # is this the problem - No\n",
    "\n",
    "        ## Evaluate the Loss\n",
    "        classification_loss = class_criterion(student_out, y) / minibatch_size\n",
    "        teacher_classification_loss = class_criterion(teacher_logit, y) / minibatch_size # is this the problem - No\n",
    "        \n",
    "        # is this the problem - No (but consistency right now is 0, we will get back to it)\n",
    "        if args.consistency:\n",
    "            consistency_weight = get_current_consistency_weight(epoch, args)\n",
    "            consistency_loss = consistency_weight * consistency_criterion(student_out, teacher_logit) / minibatch_size # is this the problem - No\n",
    "        else:\n",
    "            consistency_loss = 0\n",
    "\n",
    "        loss = classification_loss + consistency_loss # is this the problem - No\n",
    "\n",
    "        #print(\"+++++++++++++++++++++++++\")\n",
    "        #print(\"Loss\")\n",
    "        #print(loss)\n",
    "        #print(\"consistency Loss\")\n",
    "        #print(consistency_loss)\n",
    "        #print(\"+++++++++++++++++++++++++\")\n",
    "        #print(xxx)\n",
    "        ## The usual\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        global_step += 1\n",
    "        \n",
    "        update_ema_variables(model_student, model_teacher, args.ema_decay, global_step) # is this the problem\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        num = int(len(train_loader)/2)\n",
    "        if batch % num == 0:\n",
    "            print(f\"Looked at {batch * len(X)}/{len(train_loader.dataset)} samples\")\n",
    "\n",
    "    print(f\"Training Loss = {loss}, Consistency Loss = {consistency_loss}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(eval_loader, model, args):\n",
    "\n",
    "    test_acc = 0\n",
    "    class_loss = 0\n",
    "    class_criterion = nn.CrossEntropyLoss(reduction='sum', ignore_index=NO_LABEL)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for i, (input, target) in enumerate(eval_loader):\n",
    "\n",
    "            minibatch_size = len(target)\n",
    "            output = model(input)\n",
    "            softmax1 = F.softmax(output, dim=1)\n",
    "            class_loss += class_criterion(output, target) / minibatch_size\n",
    "\n",
    "            test_acc += accuracy_fn(output, target, args)\n",
    "        \n",
    "        test_acc /= len(eval_loader)\n",
    "        class_loss /= len(eval_loader)\n",
    "        \n",
    "    return test_acc, class_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]c:\\Users\\Parth\\miniconda3\\envs\\tftorch\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:1050.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 0/48000 samples\n",
      "Looked at 24000/48000 samples\n",
      "Training Loss = 0.9125328660011292, Consistency Loss = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:27<01:49, 27.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Accuracy = 65.406050955414, Teacher accuracy = 38.51512738853503\n",
      "Looked at 0/48000 samples\n",
      "Looked at 24000/48000 samples\n",
      "Training Loss = 0.6578415632247925, Consistency Loss = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:54<01:21, 27.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Accuracy = 74.20382165605095, Teacher accuracy = 72.73089171974523\n",
      "Looked at 0/48000 samples\n",
      "Looked at 24000/48000 samples\n",
      "Training Loss = 0.5941431522369385, Consistency Loss = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [01:22<00:55, 27.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Accuracy = 76.93073248407643, Teacher accuracy = 77.1297770700637\n",
      "Looked at 0/48000 samples\n",
      "Looked at 24000/48000 samples\n",
      "Training Loss = 0.3464721739292145, Consistency Loss = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [01:50<00:27, 27.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Accuracy = 81.09076433121018, Teacher accuracy = 79.04060509554141\n",
      "Looked at 0/48000 samples\n",
      "Looked at 24000/48000 samples\n",
      "Training Loss = 0.42500215768814087, Consistency Loss = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:18<00:00, 27.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Accuracy = 81.67794585987261, Teacher accuracy = 81.031050955414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "student_accuracy = []\n",
    "teacher_accuracy = []\n",
    "\n",
    "global_step = 0\n",
    "for epoch in tqdm(range(5)):\n",
    "    train(train_loader, model_student, model_teacher, optimizer, epoch, args)\n",
    "    s_acc, s_loss = validate(eval_loader, model_student, args)\n",
    "    student_accuracy.append(s_acc)\n",
    "    t_acc, t_loss = validate(eval_loader, model_teacher, args)\n",
    "    teacher_accuracy.append(t_acc)\n",
    "    \n",
    "    print(f\"Student Accuracy = {s_acc*100}, Teacher accuracy = {t_acc*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_normal(train_loader, model, optimizer, epoch, args):\n",
    "    class_criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for batch, (X,y) in enumerate(train_loader):\n",
    "        \n",
    "        output = model(X)\n",
    "        loss = class_criterion(output, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        num = int(len(train_loader)/2)\n",
    "        if batch % num == 0:\n",
    "            print(f\"Looked at {batch * len(X)}/{len(train_loader.dataset)} samples\")\n",
    "\n",
    "    print(f\"Training Loss = {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 0/60000 samples\n",
      "Looked at 30016/60000 samples\n",
      "Training Loss = 0.8393805623054504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Parth\\miniconda3\\envs\\tftorch\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      " 25%|██▌       | 1/4 [00:36<01:48, 36.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Accuracy = 65.23686305732484, Teacher accuracy = 65.23686305732484\n",
      "Looked at 0/60000 samples\n",
      "Looked at 30016/60000 samples\n",
      "Training Loss = 0.7108218669891357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [01:12<01:12, 36.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Accuracy = 73.218550955414, Teacher accuracy = 73.218550955414\n",
      "Looked at 0/60000 samples\n",
      "Looked at 30016/60000 samples\n",
      "Training Loss = 0.625800609588623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [01:48<00:36, 36.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Accuracy = 76.26393312101911, Teacher accuracy = 76.26393312101911\n",
      "Looked at 0/60000 samples\n",
      "Looked at 30016/60000 samples\n",
      "Training Loss = 0.5647347569465637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:24<00:00, 36.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Accuracy = 78.6624203821656, Teacher accuracy = 78.6624203821656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "student_accuracy = []\n",
    "teacher_accuracy = []\n",
    "\n",
    "global_step = 0\n",
    "for epoch in tqdm(range(args.epochs)):\n",
    "    train_normal(train_loader, model_2, optimizer_2, epoch, args)\n",
    "    s_acc, s_loss = validate(eval_loader, model_2, args)\n",
    "    student_accuracy.append(s_acc)\n",
    "    t_acc, t_loss = validate(eval_loader, model_2, args)\n",
    "    teacher_accuracy.append(t_acc)\n",
    "    \n",
    "    print(f\"Student Accuracy = {s_acc*100}, Teacher accuracy = {t_acc*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.exclude_unlabeled == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tftorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
