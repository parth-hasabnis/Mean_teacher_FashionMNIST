{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Dataset, Sampler, DataLoader, Subset, SubsetRandomSampler, BatchSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from helper_functions_2 import softmax_kl_loss, sigmoid_rampup, get_current_consistency_weight, linear_rampup, grouper, relabel_dataset\n",
    "import time\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNSIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=None\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=None\n",
    ")\n",
    "\n",
    "class_names = train_data.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '8')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = train_data[99]\n",
    "plt.imshow(img.squeeze(), cmap='gray')\n",
    "plt.title(str(label))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create custom labelled and unlabelled dataset from MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self, momentum, weight_decay, nesterov, epochs:int, consistency, exclude_unlabeled:bool, batch_size=64, labeled_batch_size=32, consistency_type='kl', lr=0.01, initial_lr=0.001, lr_rampup = 10, ema_decay=0.999, consistency_rampup=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "        self.nesterov = nesterov\n",
    "        self.epochs = epochs\n",
    "        self.consistency_type = consistency_type\n",
    "        self.initial_lr = initial_lr\n",
    "        self.lr_rampup = lr_rampup\n",
    "        self.consistency = consistency\n",
    "        self.ema_decay = ema_decay\n",
    "        self.labeled_batch_size = labeled_batch_size\n",
    "        self.exclude_unlabeled = exclude_unlabeled\n",
    "        self.batch_size = batch_size\n",
    "        self.consistency_rampup = consistency_rampup\n",
    "\n",
    "args = Arguments(lr=0.001, momentum=0, weight_decay=0, nesterov=False, epochs=4, exclude_unlabeled=True, consistency=0, batch_size=64, labeled_batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 1-args.labeled_batch_size/args.batch_size\n",
    "NO_LABEL = -1\n",
    "BATCH_SIZE = args.batch_size\n",
    "if(args.exclude_unlabeled == False):\n",
    "    labelled_train_dataset, unlabelled_train_dataset, labels_train, unlabels_train = train_test_split(train_data.data, train_data.targets, stratify=train_data.targets, test_size=split)\n",
    "else:\n",
    "    labelled_train_dataset = train_data.data\n",
    "    labels_train = train_data.targets\n",
    "\n",
    "class CustomTrainDataset(Dataset):\n",
    "    def __init__(self, train_data, labels=None):\n",
    "        # self.base_data = train_data/255\n",
    "        self.base_data = train_data\n",
    "        self.labels = labels\n",
    "        super().__init__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.base_data[idx]\n",
    "        if(self.labels is None):\n",
    "            label = NO_LABEL\n",
    "            # label = ToTensor(label)\n",
    "        else:\n",
    "            label = self.labels[idx]\n",
    "            label = label.item()\n",
    "        return img, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "labelled_dataset = CustomTrainDataset(train_data=labelled_train_dataset, labels=labels_train)\n",
    "if(args.exclude_unlabeled == False):\n",
    "    unlabelled_dataset = CustomTrainDataset(train_data=unlabelled_train_dataset)\n",
    "    train_dataset = torch.utils.data.ConcatDataset([labelled_dataset, unlabelled_dataset])\n",
    "else:\n",
    "    train_dataset = labelled_dataset\n",
    "\n",
    "# Temporary dataloader serves as checkpoint\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "img = train_features[0].squeeze()\n",
    "img = img.cpu().detach().numpy()\n",
    "img = img/255\n",
    "img +=  0.05*np.random.randn(img.shape[0], img.shape[0])\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoStreamBatchSampler(Sampler):\n",
    "     \n",
    "    def __init__(self, primary_indices, secondary_indices, batch_size, secondary_batch_size):\n",
    "        self.primary_indices = primary_indices\n",
    "        self.secondary_indices = secondary_indices\n",
    "        self.secondary_batch_size = secondary_batch_size\n",
    "        self.primary_batch_size = batch_size - secondary_batch_size\n",
    "\n",
    "        assert len(self.primary_indices) >= self.primary_batch_size > 0\n",
    "        assert len(self.secondary_indices) >= self.secondary_batch_size > 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "            primary_iter = np.random.permutation(self.primary_indices)\n",
    "            secondary_iter = np.random.permutation(self.secondary_indices)\n",
    "            return (\n",
    "                primary_batch + secondary_batch\n",
    "                for (primary_batch, secondary_batch)\n",
    "                in  zip(grouper(primary_iter, self.primary_batch_size),\n",
    "                        grouper(secondary_iter, self.secondary_batch_size)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.primary_indices) // self.primary_batch_size\n",
    "\n",
    "\n",
    "def create_data_loaders(train_dataset, test_dataset, args):\n",
    "    \n",
    "    labeled_idxs, unlabeled_idxs = relabel_dataset(dataset=train_dataset)\n",
    "\n",
    "    if args.exclude_unlabeled:\n",
    "        sampler = SubsetRandomSampler(labeled_idxs)\n",
    "        batch_sampler = BatchSampler(sampler, args.batch_size, drop_last=True)\n",
    "    else: \n",
    "        batch_sampler = TwoStreamBatchSampler(unlabeled_idxs, labeled_idxs, args.batch_size, args.labeled_batch_size)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                batch_sampler=batch_sampler,\n",
    "                                                pin_memory=True)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset= train_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        drop_last=False)\n",
    "\n",
    "    eval_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        drop_last=False)\n",
    "    \n",
    "    return train_loader, eval_loader\n",
    "\n",
    "train_loader, eval_loader = create_data_loaders(train_data, test_data, args)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNSITModel_V2(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=(3,3),\n",
    "                      padding=1,\n",
    "                      stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=(3,3),\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,2))\n",
    "        )\n",
    "\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=(3,3),\n",
    "                      padding=1,\n",
    "                      stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=(3,3),\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,2))\n",
    "        )\n",
    "\n",
    "\n",
    "        self.classifier= nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=hidden_units*7*7,\n",
    "                      out_features=output_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_block_2(x)\n",
    "        return self.classifier(x)\n",
    "    \n",
    "def create_models(input_shape: int, hidden_units: int, output_shape:int, ema=False):\n",
    "\n",
    "    model = FashionMNSITModel_V2(\n",
    "    input_shape=input_shape,\n",
    "    hidden_units=hidden_units,\n",
    "    output_shape=output_shape)\n",
    "\n",
    "    if ema:\n",
    "        for param in model.parameters():\n",
    "            param.detach_()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_student = create_models(input_shape=1, hidden_units=10, output_shape=len(class_names))\n",
    "model_teacher = create_models(input_shape=1, hidden_units=10, output_shape=len(class_names), ema=True)\n",
    "\n",
    "\"\"\"\n",
    "optimizer = torch.optim.SGD(model_student.parameters(), args.lr,\n",
    "                                momentum=args.momentum,\n",
    "                                weight_decay=args.weight_decay,\n",
    "                                nesterov=args.nesterov)\n",
    "\"\"\"\n",
    "\n",
    "optimizer = torch.optim.SGD(model_student.parameters(), args.lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, batch_num, batches_in_epoch, args):\n",
    "    lr = args.lr\n",
    "    epoch = epoch + batch_num / batches_in_epoch\n",
    "\n",
    "    lr = linear_rampup(epoch, args.lr_rampup) * (args.lr - args.initial_lr) + args.initial_lr\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def update_ema_variables(model, ema_model, alpha, global_step):\n",
    "    # Use the true average until the exponential average is more correct\n",
    "    alpha = min(1 - 1 / (global_step + 1), alpha)\n",
    "    for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n",
    "        ema_param.data.mul_(alpha).add_(1 - alpha, param.data)\n",
    "\n",
    "def accuracy_fn(output, target, args, train=False):\n",
    "\n",
    "    y_preds = torch.argmax(output, dim=1)\n",
    "\n",
    "    if(train):\n",
    "        y_preds = y_preds[:args.labeled_batch_size]\n",
    "        target = target[:args.labeled_batch_size]\n",
    "\n",
    "    res = sum(torch.eq(y_preds,target)).item() / len(output)\n",
    "    \n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor([1,0])\n",
    "y_hat = torch.tensor([[0,1,0],[11,0,2]])\n",
    "\n",
    "accuracy_fn(y_hat,y, args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(train_loader, model_student, model_teacher, optimizer, epoch, args):\n",
    "    global global_step\n",
    "\n",
    "    # class_criterion = nn.CrossEntropyLoss(reduction=\"sum\",ignore_index=NO_LABEL)\n",
    "    class_criterion = nn.CrossEntropyLoss()\n",
    "    if args.consistency_type == 'kl':\n",
    "        consistency_criterion = softmax_kl_loss\n",
    "\n",
    "        model_student.train()\n",
    "        model_teacher.train()\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        for batch, (X, y) in enumerate(train_loader):\n",
    "\n",
    "            # Adjust learning rate for minibatach - read more on this and minibatch/batch training\n",
    "            if(args.exclude_unlabeled == False):\n",
    "                adjust_learning_rate(optimizer, epoch, batch, len(train_loader), args)\n",
    "\n",
    "            # Add noise to student and teacher inputs: ideally data should be augmented in the data loader. look more into it\n",
    "            #student_input_var = X + 0.01*torch.randn(size=X.shape)\n",
    "            #student_input_var = student_input_var.unsqueeze(dim=1)\n",
    "            #teacher_input_var = X + 0.01*torch.randn(size=X.shape)\n",
    "            #teacher_input_var = teacher_input_var.unsqueeze(dim=1)\n",
    "\n",
    "            student_input_var = X\n",
    "            teacher_input_var = X\n",
    "\n",
    "            minibatch_size = len(y)\n",
    "\n",
    "            # Forward Pass\n",
    "            student_out = model_student(student_input_var)\n",
    "            teacher_out = model_teacher(teacher_input_var)\n",
    "            \n",
    "            teacher_logit = teacher_out.detach().data\n",
    "\n",
    "            ## Evaluate the Loss\n",
    "            classification_loss = class_criterion(student_out, y) / minibatch_size\n",
    "            teacher_classification_loss = class_criterion(teacher_logit, y) / minibatch_size\n",
    "            \n",
    "\n",
    "            if args.consistency:\n",
    "                consistency_weight = get_current_consistency_weight(epoch, args)\n",
    "                consistency_loss = consistency_weight * consistency_criterion(student_out, teacher_logit) / minibatch_size\n",
    "            else:\n",
    "                consistency_loss = 0\n",
    "\n",
    "            loss = classification_loss + consistency_loss\n",
    "\n",
    "            #print(\"+++++++++++++++++++++++++\")\n",
    "            #print(\"Loss\")\n",
    "            #print(loss)\n",
    "            #print(\"consistency Loss\")\n",
    "            #print(consistency_loss)\n",
    "            #print(\"+++++++++++++++++++++++++\")\n",
    "            #print(xxx)\n",
    "            ## The usual\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "            \n",
    "            update_ema_variables(model_student, model_teacher, args.ema_decay, global_step)\n",
    "\n",
    "            end = time.time()\n",
    "\n",
    "            num = int(len(train_loader)/2)\n",
    "            if batch % num == 0:\n",
    "                print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
    "\n",
    "        print(f\"Training Loss = {loss}, learning rate = {optimizer.state_dict}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(eval_loader, model, args):\n",
    "\n",
    "    test_acc = 0\n",
    "    class_loss = 0\n",
    "    class_criterion = nn.CrossEntropyLoss(size_average=False, ignore_index=NO_LABEL)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for i, (input, target) in enumerate(eval_loader):\n",
    "\n",
    "            minibatch_size = len(target)\n",
    "            output = model(input)\n",
    "            softmax1 = F.softmax(output, dim=1)\n",
    "            class_loss += class_criterion(output, target) / minibatch_size\n",
    "\n",
    "            test_acc += accuracy_fn(output, target, args)\n",
    "        \n",
    "        test_acc /= len(eval_loader)\n",
    "        class_loss /= len(eval_loader)\n",
    "        \n",
    "    return test_acc, class_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 0/60000 samples\n",
      "Looked at 30016/60000 samples\n",
      "Training Loss = 0.07203298807144165, learning rate = <bound method Optimizer.state_dict of SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.001\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Parth\\miniconda3\\envs\\tftorch\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      " 25%|██▌       | 1/4 [00:40<02:02, 40.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Accuracy = 4.856687898089172, Teacher accuracy = 4.806926751592357\n",
      "Looked at 0/60000 samples\n",
      "Looked at 30016/60000 samples\n",
      "Training Loss = 0.07203090935945511, learning rate = <bound method Optimizer.state_dict of SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.001\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [01:21<01:21, 40.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Accuracy = 5.0358280254777075, Teacher accuracy = 4.886544585987261\n",
      "Looked at 0/60000 samples\n",
      "Looked at 30016/60000 samples\n",
      "Training Loss = 0.07202887535095215, learning rate = <bound method Optimizer.state_dict of SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.001\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [02:01<00:40, 40.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Accuracy = 5.224920382165605, Teacher accuracy = 5.0358280254777075\n",
      "Looked at 0/60000 samples\n",
      "Looked at 30016/60000 samples\n",
      "Training Loss = 0.07202684879302979, learning rate = <bound method Optimizer.state_dict of SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.001\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:42<00:00, 40.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Accuracy = 5.334394904458599, Teacher accuracy = 5.214968152866242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "student_accuracy = []\n",
    "teacher_accuracy = []\n",
    "\n",
    "global_step = 0\n",
    "for epoch in tqdm(range(args.epochs)):\n",
    "    train(train_loader, model_student, model_teacher, optimizer, epoch, args)\n",
    "    s_acc, s_loss = validate(eval_loader, model_student, args)\n",
    "    student_accuracy.append(s_acc)\n",
    "    t_acc, t_loss = validate(eval_loader, model_teacher, args)\n",
    "    teacher_accuracy.append(t_acc)\n",
    "    \n",
    "    print(f\"Student Accuracy = {s_acc*100}, Teacher accuracy = {t_acc*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04856687898089172, 0.05035828025477707, 0.05224920382165605, 0.05334394904458599]\n",
      "[0.04806926751592357, 0.04886544585987261, 0.05035828025477707, 0.05214968152866242]\n"
     ]
    }
   ],
   "source": [
    "print(student_accuracy)\n",
    "print(teacher_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = next(iter(eval_loader))\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = next(iter(train_loader))\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3, 5, 3, 3, 5, 3, 3, 3, 3, 3]),\n",
       " tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model_student(X)\n",
    "y_preds = torch.argmax(output, dim=1)\n",
    "y_preds[:10], y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3, 5, 3, 3, 5, 3, 3, 3, 3, 3]),\n",
       " tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model_student(X)\n",
    "output = F.softmax(output, dim=1)\n",
    "y_preds = torch.argmax(output, dim=1)\n",
    "y_preds[:10], y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tftorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
