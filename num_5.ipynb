{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Dataset, Sampler, DataLoader, Subset, SubsetRandomSampler, BatchSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from helper_functions_2 import softmax_kl_loss, sigmoid_rampup, get_current_consistency_weight, linear_rampup, grouper, relabel_dataset\n",
    "import time\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNSIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=None\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=None\n",
    ")\n",
    "\n",
    "class_names = train_data.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '8')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = train_data[99]\n",
    "plt.imshow(img.squeeze(), cmap='gray')\n",
    "plt.title(str(label))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create custom labelled and unlabelled dataset from MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self, momentum, weight_decay, nesterov, epochs:int, consistency, exclude_unlabeled:bool, batch_size=64, labeled_batch_size=32, consistency_type='kl', lr=0.01, initial_lr=0.001, lr_rampup = 10, ema_decay=0.999, consistency_rampup=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "        self.nesterov = nesterov\n",
    "        self.epochs = epochs\n",
    "        self.consistency_type = consistency_type\n",
    "        self.initial_lr = initial_lr\n",
    "        self.lr_rampup = lr_rampup\n",
    "        self.consistency = consistency\n",
    "        self.ema_decay = ema_decay\n",
    "        self.labeled_batch_size = labeled_batch_size\n",
    "        self.exclude_unlabeled = exclude_unlabeled\n",
    "        self.batch_size = batch_size\n",
    "        self.consistency_rampup = consistency_rampup\n",
    "\n",
    "args = Arguments(lr=0.01, momentum=0, weight_decay=0, nesterov=False, epochs=4, exclude_unlabeled=True, consistency=0, batch_size=32, labeled_batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 1-args.labeled_batch_size/args.batch_size\n",
    "NO_LABEL = -1\n",
    "BATCH_SIZE = args.batch_size\n",
    "if(args.exclude_unlabeled == False):\n",
    "    labelled_train_dataset, unlabelled_train_dataset, labels_train, unlabels_train = train_test_split(train_data.data, train_data.targets, stratify=train_data.targets, test_size=split)\n",
    "else:\n",
    "    labelled_train_dataset = train_data.data\n",
    "    labels_train = train_data.targets\n",
    "\n",
    "class CustomTrainDataset(Dataset):\n",
    "    def __init__(self, train_data, labels=None):\n",
    "        # self.base_data = train_data/255\n",
    "        self.base_data = train_data\n",
    "        self.labels = labels\n",
    "        super().__init__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.base_data[idx]\n",
    "        if(self.labels is None):\n",
    "            label = NO_LABEL\n",
    "            # label = ToTensor(label)\n",
    "        else:\n",
    "            label = self.labels[idx]\n",
    "            label = label.item()\n",
    "        return img, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "UFuncTypeError",
     "evalue": "Cannot cast ufunc 'add' output from dtype('float64') to dtype('uint8') with casting rule 'same_kind'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-124-3f32fe1c91e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# img = img/255\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mimg\u001b[0m \u001b[1;33m+=\u001b[0m  \u001b[1;36m0.05\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"gray\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUFuncTypeError\u001b[0m: Cannot cast ufunc 'add' output from dtype('float64') to dtype('uint8') with casting rule 'same_kind'"
     ]
    }
   ],
   "source": [
    "labelled_dataset = CustomTrainDataset(train_data=labelled_train_dataset, labels=labels_train)\n",
    "if(args.exclude_unlabeled == False):\n",
    "    unlabelled_dataset = CustomTrainDataset(train_data=unlabelled_train_dataset)\n",
    "    train_dataset = torch.utils.data.ConcatDataset([labelled_dataset, unlabelled_dataset])\n",
    "else:\n",
    "    train_dataset = labelled_dataset\n",
    "\n",
    "# Temporary dataloader serves as checkpoint\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "img = train_features[0].squeeze()\n",
    "img = img.cpu().detach().numpy()\n",
    "# img = img/255\n",
    "img +=  0.05*np.random.randn(img.shape[0], img.shape[0])\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoStreamBatchSampler(Sampler):\n",
    "     \n",
    "    def __init__(self, primary_indices, secondary_indices, batch_size, secondary_batch_size):\n",
    "        self.primary_indices = primary_indices\n",
    "        self.secondary_indices = secondary_indices\n",
    "        self.secondary_batch_size = secondary_batch_size\n",
    "        self.primary_batch_size = batch_size - secondary_batch_size\n",
    "\n",
    "        assert len(self.primary_indices) >= self.primary_batch_size > 0\n",
    "        assert len(self.secondary_indices) >= self.secondary_batch_size > 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "            primary_iter = np.random.permutation(self.primary_indices)\n",
    "            secondary_iter = np.random.permutation(self.secondary_indices)\n",
    "            return (\n",
    "                primary_batch + secondary_batch\n",
    "                for (primary_batch, secondary_batch)\n",
    "                in  zip(grouper(primary_iter, self.primary_batch_size),\n",
    "                        grouper(secondary_iter, self.secondary_batch_size)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.primary_indices) // self.primary_batch_size\n",
    "\n",
    "\n",
    "def create_data_loaders(train_dataset, test_dataset, args):\n",
    "    \n",
    "    labeled_idxs, unlabeled_idxs = relabel_dataset(dataset=train_dataset)\n",
    "\n",
    "    if args.exclude_unlabeled:\n",
    "        sampler = SubsetRandomSampler(labeled_idxs)\n",
    "        batch_sampler = BatchSampler(sampler, args.batch_size, drop_last=True)\n",
    "    else: \n",
    "        batch_sampler = TwoStreamBatchSampler(unlabeled_idxs, labeled_idxs, args.batch_size, args.labeled_batch_size)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                batch_sampler=batch_sampler,\n",
    "                                                pin_memory=True)\n",
    "\n",
    "    eval_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        drop_last=False)\n",
    "    \n",
    "    return train_loader, eval_loader\n",
    "\n",
    "train_loader, eval_loader = create_data_loaders(train_dataset, test_data, args)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNSITModel_V2(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=(3,3),\n",
    "                      padding=1,\n",
    "                      stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=(3,3),\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,2))\n",
    "        )\n",
    "\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=(3,3),\n",
    "                      padding=1,\n",
    "                      stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=(3,3),\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,2))\n",
    "        )\n",
    "\n",
    "\n",
    "        self.classifier= nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=hidden_units*7*7,\n",
    "                      out_features=output_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_block_2(x)\n",
    "        return self.classifier(x)\n",
    "    \n",
    "def create_models(input_shape: int, hidden_units: int, output_shape:int, ema=False):\n",
    "\n",
    "    model = FashionMNSITModel_V2(\n",
    "    input_shape=input_shape,\n",
    "    hidden_units=hidden_units,\n",
    "    output_shape=output_shape)\n",
    "\n",
    "    if ema:\n",
    "        for param in model.parameters():\n",
    "            param.detach_()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_student = create_models(input_shape=1, hidden_units=10, output_shape=len(class_names))\n",
    "model_teacher = create_models(input_shape=1, hidden_units=10, output_shape=len(class_names), ema=True)\n",
    "\n",
    "\"\"\"\n",
    "optimizer = torch.optim.SGD(model_student.parameters(), args.lr,\n",
    "                                momentum=args.momentum,\n",
    "                                weight_decay=args.weight_decay,\n",
    "                                nesterov=args.nesterov)\n",
    "\"\"\"\n",
    "\n",
    "optimizer = torch.optim.SGD(model_student.parameters(), args.lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, batch_num, batches_in_epoch, args):\n",
    "    lr = args.lr\n",
    "    epoch = epoch + batch_num / batches_in_epoch\n",
    "\n",
    "    lr = linear_rampup(epoch, args.lr_rampup) * (args.lr - args.initial_lr) + args.initial_lr\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def update_ema_variables(model, ema_model, alpha, global_step):\n",
    "    # Use the true average until the exponential average is more correct\n",
    "    alpha = min(1 - 1 / (global_step + 1), alpha)\n",
    "    for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n",
    "        ema_param.data.mul_(alpha).add_(1 - alpha, param.data)\n",
    "\n",
    "def accuracy_fn(output, target, args, train=False):\n",
    "\n",
    "    y_preds = torch.argmax(output, dim=1)\n",
    "\n",
    "    if(train):\n",
    "        y_preds = y_preds[:args.labeled_batch_size]\n",
    "        target = target[:args.labeled_batch_size]\n",
    "\n",
    "    res = sum(torch.eq(y_preds,target)).item() / len(output)\n",
    "    \n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor([1,0])\n",
    "y_hat = torch.tensor([[0,1,0],[11,0,2]])\n",
    "\n",
    "accuracy_fn(y_hat,y, args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(train_loader, model_student, model_teacher, optimizer, epoch, args):\n",
    "    global global_step\n",
    "\n",
    "    # class_criterion = nn.CrossEntropyLoss(reduction=\"sum\",ignore_index=NO_LABEL)\n",
    "    class_criterion = nn.CrossEntropyLoss()\n",
    "    if args.consistency_type == 'kl':\n",
    "        consistency_criterion = softmax_kl_loss\n",
    "\n",
    "        model_student.train()\n",
    "        model_teacher.train()\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        for batch, (X, y) in enumerate(train_loader):\n",
    "\n",
    "            # Adjust learning rate for minibatach - read more on this and minibatch/batch training\n",
    "            if(args.exclude_unlabeled == False):\n",
    "                adjust_learning_rate(optimizer, epoch, batch, len(train_loader), args)\n",
    "\n",
    "            # Add noise to student and teacher inputs: ideally data should be augmented in the data loader. look more into it\n",
    "            #student_input_var = X + 0.01*torch.randn(size=X.shape)\n",
    "            #student_input_var = student_input_var.unsqueeze(dim=1)\n",
    "            #teacher_input_var = X + 0.01*torch.randn(size=X.shape)\n",
    "            #teacher_input_var = teacher_input_var.unsqueeze(dim=1)\n",
    "\n",
    "            student_input_var = X\n",
    "            teacher_input_var = X\n",
    "\n",
    "            minibatch_size = len(y)\n",
    "\n",
    "            # Forward Pass\n",
    "            student_out = model_student(student_input_var)\n",
    "            teacher_out = model_teacher(teacher_input_var)\n",
    "            \n",
    "            teacher_logit = teacher_out.detach().data\n",
    "\n",
    "            ## Evaluate the Loss\n",
    "            classification_loss = class_criterion(student_out, y) / minibatch_size\n",
    "            teacher_classification_loss = class_criterion(teacher_logit, y) / minibatch_size\n",
    "            \n",
    "\n",
    "            if args.consistency:\n",
    "                consistency_weight = get_current_consistency_weight(epoch, args)\n",
    "                consistency_loss = consistency_weight * consistency_criterion(student_out, teacher_logit) / minibatch_size\n",
    "            else:\n",
    "                consistency_loss = 0\n",
    "\n",
    "            loss = classification_loss + consistency_loss\n",
    "\n",
    "            #print(\"+++++++++++++++++++++++++\")\n",
    "            #print(\"Loss\")\n",
    "            #print(loss)\n",
    "            #print(\"consistency Loss\")\n",
    "            #print(consistency_loss)\n",
    "            #print(\"+++++++++++++++++++++++++\")\n",
    "            #print(xxx)\n",
    "            ## The usual\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "            \n",
    "            update_ema_variables(model_student, model_teacher, args.ema_decay, global_step)\n",
    "\n",
    "            end = time.time()\n",
    "\n",
    "            num = int(batch/10)\n",
    "            if batch % 100 == 0:\n",
    "                print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
    "\n",
    "        print(f\"Training Loss = {loss}, learning rate = {optimizer.state_dict}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(eval_loader, model, args):\n",
    "\n",
    "    test_acc = 0\n",
    "    class_loss = 0\n",
    "    class_criterion = nn.CrossEntropyLoss(size_average=False, ignore_index=NO_LABEL)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for i, (input, target) in enumerate(eval_loader):\n",
    "\n",
    "            minibatch_size = len(target)\n",
    "            output = model(input)\n",
    "            softmax1 = F.softmax(output, dim=1)\n",
    "            class_loss += class_criterion(output, target) / minibatch_size\n",
    "\n",
    "            test_acc += accuracy_fn(output, target, args)\n",
    "        \n",
    "        test_acc /= len(eval_loader)\n",
    "        class_loss /= len(eval_loader)\n",
    "        \n",
    "    return test_acc, class_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [10, 1, 3, 3], but got 3-dimensional input of size [32, 28, 28] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-132-82ce49bacce8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_student\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_teacher\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0ms_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_student\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mstudent_accuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-130-b92a7fcc50f5>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_loader, model_student, model_teacher, optimizer, epoch, args)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[1;31m# Forward Pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0mstudent_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_student\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudent_input_var\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m             \u001b[0mteacher_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_teacher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mteacher_input_var\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Parth\\miniconda3\\envs\\tftorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-126-e69d9d30197c>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_block_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_block_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Parth\\miniconda3\\envs\\tftorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Parth\\miniconda3\\envs\\tftorch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Parth\\miniconda3\\envs\\tftorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Parth\\miniconda3\\envs\\tftorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Parth\\miniconda3\\envs\\tftorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    442\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[1;32m--> 443\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [10, 1, 3, 3], but got 3-dimensional input of size [32, 28, 28] instead"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "student_accuracy = []\n",
    "teacher_accuracy = []\n",
    "\n",
    "global_step = 0\n",
    "for epoch in tqdm(range(args.epochs)):\n",
    "    train(train_loader, model_student, model_teacher, optimizer, epoch, args)\n",
    "    s_acc, s_loss = validate(eval_loader, model_student, args)\n",
    "    student_accuracy.append(s_acc)\n",
    "    t_acc, t_loss = validate(eval_loader, model_teacher, args)\n",
    "    teacher_accuracy.append(t_acc)\n",
    "    \n",
    "    print(f\"Student Accuracy = {s_acc*100}, Teacher accuracy = {t_acc*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10041799363057324, 0.10041799363057324, 0.10041799363057324, 0.10041799363057324]\n",
      "[0.10041799363057324, 0.10041799363057324, 0.10041799363057324, 0.10041799363057324]\n"
     ]
    }
   ],
   "source": [
    "print(student_accuracy)\n",
    "print(teacher_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1, 28, 28]), torch.Size([32]))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = next(iter(eval_loader))\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 28, 28]), torch.Size([32]))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = next(iter(train_loader))\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tftorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
